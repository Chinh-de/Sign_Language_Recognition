{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import mediapipe as mp\n",
    "from pathlib import Path\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo MediaPipe\n",
    "mp_pose = mp.solutions.pose.Pose(min_detection_confidence=0.6, min_tracking_confidence=0.6)\n",
    "mp_hands = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "yolov3-tiny summary: 48 layers, 8849182 parameters, 0 gradients, 13.2 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetectMultiBackend(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (11): ZeroPad2d((0, 1, 0, 1))\n",
       "      (12): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (13): Conv(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (14): Conv(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (15): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (18): Concat()\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Detect(\n",
       "        (m): ModuleList(\n",
       "          (0): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load YOLOv3 model\n",
    "device = torch.device(\"cpu\")\n",
    "yolo_model = DetectMultiBackend(\"weights/yolov3-tiny.pt\")\n",
    "yolo_model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục lưu kết quả\n",
    "output_dir = Path(\"runs/detect/frames\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "video_output_path = \"runs/detect/output_video.mp4\"\n",
    "json_output_path = \"runs/detect/sign_space.json\"\n",
    "json_output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kích thước YOLO yêu cầu (mặc định 640x640)\n",
    "YOLO_IMG_SIZE = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_frame(frame):\n",
    "    \"\"\"Resize frame về kích thước YOLO yêu cầu nhưng vẫn lưu tỷ lệ ảnh gốc.\"\"\"\n",
    "    original_size = frame.shape[:2]  # (height, width)\n",
    "    resized_frame = cv2.resize(frame, (YOLO_IMG_SIZE, YOLO_IMG_SIZE))\n",
    "    return resized_frame, original_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_person(model, frame):\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = torch.from_numpy(img).to(device).permute(2, 0, 1).float().div(255.0).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(img)\n",
    "    \n",
    "    detections = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=[0])\n",
    "    \n",
    "    print(f\"[DEBUG] Detections: {detections}\")  # Kiểm tra kết quả YOLO\n",
    "\n",
    "\n",
    "    if detections[0] is not None:\n",
    "        return detections[0].cpu().numpy()\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_and_hand_landmarks(frame):\n",
    "    \"\"\"Lấy tọa độ pose và bàn tay từ MediaPipe\"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Xử lý pose landmarks\n",
    "    results_pose = mp_pose.process(frame_rgb)\n",
    "    pose_landmarks = []\n",
    "    if results_pose.pose_landmarks:\n",
    "        pose_landmarks = [(lm.x * frame.shape[1], lm.y * frame.shape[0]) for lm in results_pose.pose_landmarks.landmark]\n",
    "    \n",
    "    # Xử lý hand landmarks\n",
    "    results_hands = mp_hands.\n",
    "    \n",
    "    \n",
    "    (frame_rgb)\n",
    "\n",
    "    left_hand_landmarks = []\n",
    "    right_hand_landmarks = []\n",
    "    \n",
    "    if results_hands.multi_hand_landmarks and results_hands.multi_handedness:\n",
    "        for hand_landmarks, handedness in zip(results_hands.multi_hand_landmarks, results_hands.multi_handedness):\n",
    "            label = handedness.classification[0].label  # \"Left\" hoặc \"Right\"\n",
    "            landmarks = [(lm.x * frame.shape[1], lm.y * frame.shape[0]) for lm in hand_landmarks.landmark]\n",
    "            \n",
    "            # Quan trọng: Đây là góc nhìn của camera, nên \"Left\" thực sự là tay phải của người\n",
    "            # và \"Right\" thực sự là tay trái của người\n",
    "            if label == \"Left\":  # Tay phải của người (từ góc nhìn camera)\n",
    "                right_hand_landmarks = landmarks\n",
    "            elif label == \"Right\":  # Tay trái của người (từ góc nhìn camera)\n",
    "                left_hand_landmarks = landmarks\n",
    "\n",
    "    return pose_landmarks, left_hand_landmarks, right_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_head_unit(pose_landmarks, person_box, original_size):\n",
    "    height, width = original_size\n",
    "    x1, y1, x2, y2 = person_box[:4]\n",
    "    \n",
    "    if pose_landmarks and len(pose_landmarks) >= 6:\n",
    "        left_eye, right_eye = pose_landmarks[2], pose_landmarks[5]\n",
    "        head_unit = np.sqrt((left_eye[0] - right_eye[0])**2 + (left_eye[1] - right_eye[1])**2)\n",
    "    else:\n",
    "        head_unit = ((x2 - x1) / width + (y2 - y1) / height) / 2 * height / 6\n",
    "    \n",
    "    # print(f\"[INFO] Head Unit: {head_unit}\")  # Debug head_unit\n",
    "    return head_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hand_bbox_from_mediapipe(hand_landmarks, img_shape, padding=15):\n",
    "    \"\"\"Tính toán bounding box cho bàn tay dựa trên toàn bộ landmark, không chỉ cổ tay\"\"\"\n",
    "    if not hand_landmarks:\n",
    "        return None\n",
    "\n",
    "    height, width = img_shape[:2]\n",
    "    \n",
    "    x_coords = [int(landmark[0]) for landmark in hand_landmarks]\n",
    "    y_coords = [int(landmark[1]) for landmark in hand_landmarks]\n",
    "\n",
    "    x_min, y_min = max(0, min(x_coords) - padding), max(0, min(y_coords) - padding)\n",
    "    x_max, y_max = min(width, max(x_coords) + padding), min(height, max(y_coords) + padding)\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anchor_boxes(sign_space, hand_boxes, original_size, resized_size, confidence=0.5):\n",
    "    \"\"\"\n",
    "    Tính toán các hộp neo chuẩn hóa cho không gian ký hiệu và bàn tay\n",
    "    \n",
    "    Parameters:\n",
    "    sign_space (list): Bounding box của không gian ký hiệu [x1, y1, x2, y2]\n",
    "    hand_boxes (dict): Dictionary chứa bounding box của tay trái và phải\n",
    "    original_size (tuple): Kích thước gốc của video (height, width)\n",
    "    resized_size (tuple): Kích thước của frame sau khi resize (height, width)\n",
    "    confidence (float): Độ tin cậy của dự đoán (mặc định 0.5)\n",
    "    \n",
    "    Returns:\n",
    "    list: Danh sách các hộp neo chuẩn hóa\n",
    "    \"\"\"\n",
    "    anchor_boxes = []\n",
    "    \n",
    "    # Thêm hộp neo cho không gian ký hiệu nếu có\n",
    "    if sign_space:\n",
    "        normalized_sign_space = normalize_bbox(sign_space, original_size, resized_size)\n",
    "        anchor_boxes.append({\n",
    "            \"type\": \"sign_space\",\n",
    "            \"bbox\": normalized_sign_space,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "    \n",
    "    # Thêm hộp neo cho tay trái nếu có\n",
    "    if hand_boxes.get(\"left\"):\n",
    "        normalized_left_hand = normalize_bbox(hand_boxes[\"left\"], original_size, resized_size)\n",
    "        anchor_boxes.append({\n",
    "            \"type\": \"left_hand\",\n",
    "            \"bbox\": normalized_left_hand,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "    \n",
    "    # Thêm hộp neo cho tay phải nếu có\n",
    "    if hand_boxes.get(\"right\"):\n",
    "        normalized_right_hand = normalize_bbox(hand_boxes[\"right\"], original_size, resized_size)\n",
    "        anchor_boxes.append({\n",
    "            \"type\": \"right_hand\",\n",
    "            \"bbox\": normalized_right_hand,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "    \n",
    "    return anchor_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox(bbox, original_size, resized_size, confidence=0.5):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa bounding box từ frame đã resize về tọa độ chuẩn hóa [x_center, y_center, width, height]\n",
    "    dựa trên kích thước gốc của video\n",
    "    \n",
    "    Parameters:\n",
    "    bbox (list): Bounding box ở định dạng [x1, y1, x2, y2] trên frame đã resize\n",
    "    original_size (tuple): Kích thước gốc của video (height, width)\n",
    "    resized_size (tuple): Kích thước của frame sau khi resize (height, width)\n",
    "    \n",
    "    Returns:\n",
    "    list: Bounding box chuẩn hóa ở định dạng [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    original_height, original_width = original_size\n",
    "    resized_height, resized_width = resized_size\n",
    "    \n",
    "    # Chuyển đổi tọa độ từ frame đã resize về tỷ lệ tương ứng trong frame gốc\n",
    "    x1_ratio = x1 / resized_width\n",
    "    y1_ratio = y1 / resized_height\n",
    "    x2_ratio = x2 / resized_width\n",
    "    y2_ratio = y2 / resized_height\n",
    "    \n",
    "    # Tính toán tọa độ trung tâm và kích thước (đã được chuẩn hóa)\n",
    "    x_center_norm = (x1_ratio + x2_ratio) / 2.0\n",
    "    y_center_norm = (y1_ratio + y2_ratio) / 2.0\n",
    "    width_norm = x2_ratio - x1_ratio\n",
    "    height_norm = y2_ratio - y1_ratio\n",
    "    \n",
    "    return [x_center_norm, y_center_norm, width_norm, height_norm,confidence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_sign_space(frame, pose_landmarks, head_unit):\n",
    "    if not pose_landmarks or head_unit is None:\n",
    "        return None\n",
    "    nose, left_eye, right_eye = pose_landmarks[0], pose_landmarks[2], pose_landmarks[5]\n",
    "    # Kích thước box (theo tài liệu là 7 - 8)\n",
    "    width = 7 * head_unit\n",
    "    height = 8 * head_unit \n",
    "    \n",
    "    # Dùng trung bình giữa mũi và mắt để ổn định center_x\n",
    "    center_x = int(nose[0])  # Tâm ngang chính xác theo mũi\n",
    "    # center_y = int((nose[1] + left_eye[1]) / 2)  # Trung bình giữa mũi và mắt trái\n",
    "    center_y = int(left_eye[1])  # Định vị theo mắt trái\n",
    "\n",
    "    \n",
    "    x1 = int(center_x - width / 2)\n",
    "    # Lùi lên 1.5 đơn vị đầu (Theo tài liệu 0.5)\n",
    "    y1 = int(center_y - 1.5 * head_unit)  # Điều chỉnh vị trí y1\n",
    "    x2 = int(center_x + width / 2)\n",
    "    y2 = min(frame.shape[0] - 1, int(center_y + 7.5 * head_unit))  # Cạnh dưới cách mắt trái 7.5 head_unit\n",
    "    \n",
    "    h, w, _ = frame.shape\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(w - 1, x2), min(h - 1, y2)\n",
    "    print(f\"[INFO] Sign Space Box: ({x1}, {y1}), ({x2}, {y2})\")  # Debug bbox\n",
    "    return [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox_on_image(img, bbox, color=(0, 255, 0), thickness=2, label=\"Hand Anchor\"):\n",
    "    \"\"\"Vẽ bounding box lên hình ảnh\"\"\"\n",
    "    if bbox is None:\n",
    "        return img\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, frame_index):\n",
    "    resized_frame, original_size = resize_frame(frame)\n",
    "    detections = detect_person(yolo_model, resized_frame)\n",
    "    \n",
    "    if len(detections) == 0:\n",
    "        print(\"[DEBUG] Không phát hiện người trong frame!\")\n",
    "        return frame, None\n",
    "\n",
    "    # Lấy pose + landmark tay từ MediaPipe\n",
    "    pose_landmarks, left_hand_landmarks, right_hand_landmarks = get_pose_and_hand_landmarks(resized_frame)\n",
    "    \n",
    "    head_unit = calculate_head_unit(pose_landmarks, detections[0], original_size)\n",
    "    sign_space = calculate_normalized_sign_space(resized_frame, pose_landmarks, head_unit)\n",
    "\n",
    "    # Tính toán bounding box tay\n",
    "    left_hand_box = calculate_hand_bbox_from_mediapipe(left_hand_landmarks, resized_frame.shape) if left_hand_landmarks else None\n",
    "    right_hand_box = calculate_hand_bbox_from_mediapipe(right_hand_landmarks, resized_frame.shape) if right_hand_landmarks else None\n",
    "\n",
    "    # Vẽ hộp không gian ký hiệu\n",
    "    if sign_space:\n",
    "        x1, y1, x2, y2 = sign_space\n",
    "        cv2.rectangle(resized_frame, (x1, y1), (x2, y2), (255, 255, 0), 2)  # Màu xanh dương nhạt\n",
    "\n",
    "    # Vẽ hộp tay\n",
    "    if left_hand_box:\n",
    "        draw_bbox_on_image(resized_frame, left_hand_box, color=(255, 0, 0), label=\"Left Hand\")\n",
    "    if right_hand_box:\n",
    "        draw_bbox_on_image(resized_frame, right_hand_box, color=(0, 0, 255), label=\"Right Hand\")\n",
    "\n",
    "    print(f\"[DEBUG] Sign Space: {sign_space}\")  # Kiểm tra tọa độ sign space\n",
    "\n",
    "\n",
    "    return resized_frame, sign_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Không thể mở video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Lấy thông tin video để tạo video output\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Tạo video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(video_output_path, fourcc, fps, (YOLO_IMG_SIZE, YOLO_IMG_SIZE))\n",
    "    \n",
    "    frame_index = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Xử lý frame\n",
    "        processed_frame, sign_space = process_frame(frame, frame_index)\n",
    "        \n",
    "        # Ghi frame đã xử lý vào video output\n",
    "        out.write(processed_frame)\n",
    "        \n",
    "        # Hiển thị frame\n",
    "        cv2.imshow('Sign Space & Hand Detection', processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Lưu thông tin sign_space vào json_output nếu có\n",
    "        if sign_space:\n",
    "            json_output[str(frame_index)] = sign_space\n",
    "        \n",
    "        frame_index += 1\n",
    "\n",
    "    # Giải phóng tài nguyên\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Lưu json_output ra file\n",
    "    with open(json_output_path, 'w') as f:\n",
    "        json.dump(json_output, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Detections: [tensor([[1.50357e+02, 1.07487e+02, 5.15583e+02, 5.68277e+02, 2.79987e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (137, 82), (516, 571)\n",
      "[DEBUG] Sign Space: [137, 82, 516, 571]\n",
      "[DEBUG] Detections: [tensor([[1.53739e+02, 1.08097e+02, 5.10761e+02, 5.72974e+02, 2.61660e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (136, 81), (517, 570)\n",
      "[DEBUG] Sign Space: [136, 81, 517, 570]\n",
      "[DEBUG] Detections: [tensor([[1.52560e+02, 1.08857e+02, 5.09896e+02, 5.72519e+02, 2.57786e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (136, 81), (517, 571)\n",
      "[DEBUG] Sign Space: [136, 81, 517, 571]\n",
      "[DEBUG] Detections: [tensor([[1.52601e+02, 1.08680e+02, 5.09789e+02, 5.72566e+02, 2.56017e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (137, 81), (518, 571)\n",
      "[DEBUG] Sign Space: [137, 81, 518, 571]\n",
      "[DEBUG] Detections: [tensor([[1.53298e+02, 1.07742e+02, 5.11598e+02, 5.69587e+02, 2.62489e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (137, 82), (518, 571)\n",
      "[DEBUG] Sign Space: [137, 82, 518, 571]\n",
      "[DEBUG] Detections: [tensor([[1.53298e+02, 1.07742e+02, 5.11598e+02, 5.69587e+02, 2.62489e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (139, 83), (518, 570)\n",
      "[DEBUG] Sign Space: [139, 83, 518, 570]\n",
      "[DEBUG] Detections: [tensor([[1.53298e+02, 1.07742e+02, 5.11598e+02, 5.69587e+02, 2.62489e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (141, 84), (516, 567)\n",
      "[DEBUG] Sign Space: [141, 84, 516, 567]\n",
      "[DEBUG] Detections: [tensor([[1.49627e+02, 1.03300e+02, 5.13822e+02, 5.72256e+02, 2.79496e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (143, 84), (516, 565)\n",
      "[DEBUG] Sign Space: [143, 84, 516, 565]\n",
      "[DEBUG] Detections: [tensor([[1.49627e+02, 1.03300e+02, 5.13822e+02, 5.72256e+02, 2.79496e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (143, 85), (516, 564)\n",
      "[DEBUG] Sign Space: [143, 85, 516, 564]\n",
      "[DEBUG] Detections: [tensor([[1.50661e+02, 1.06513e+02, 5.12098e+02, 5.68166e+02, 2.76440e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (143, 84), (516, 563)\n",
      "[DEBUG] Sign Space: [143, 84, 516, 563]\n",
      "[DEBUG] Detections: [tensor([[1.53830e+02, 1.06837e+02, 5.06375e+02, 5.69680e+02, 2.75038e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (143, 83), (516, 562)\n",
      "[DEBUG] Sign Space: [143, 83, 516, 562]\n",
      "[DEBUG] Detections: [tensor([[1.49034e+02, 1.06714e+02, 5.14153e+02, 5.66490e+02, 2.96620e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (143, 82), (516, 561)\n",
      "[DEBUG] Sign Space: [143, 82, 516, 561]\n",
      "[DEBUG] Detections: [tensor([[1.55945e+02, 1.09764e+02, 5.11180e+02, 5.67113e+02, 2.80867e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (143, 82), (516, 561)\n",
      "[DEBUG] Sign Space: [143, 82, 516, 561]\n",
      "[DEBUG] Detections: [tensor([[1.55526e+02, 1.09344e+02, 5.11990e+02, 5.67232e+02, 3.00123e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (145, 82), (516, 560)\n",
      "[DEBUG] Sign Space: [145, 82, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.55526e+02, 1.09344e+02, 5.11990e+02, 5.67232e+02, 3.00123e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (146, 82), (517, 559)\n",
      "[DEBUG] Sign Space: [146, 82, 517, 559]\n",
      "[DEBUG] Detections: [tensor([[1.55548e+02, 1.09352e+02, 5.11958e+02, 5.67218e+02, 3.00152e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (146, 82), (517, 558)\n",
      "[DEBUG] Sign Space: [146, 82, 517, 558]\n",
      "[DEBUG] Detections: [tensor([[1.53008e+02, 1.00648e+02, 5.12884e+02, 5.74210e+02, 2.73398e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (148, 84), (517, 557)\n",
      "[DEBUG] Sign Space: [148, 84, 517, 557]\n",
      "[DEBUG] Detections: [tensor([[1.53008e+02, 1.00648e+02, 5.12884e+02, 5.74210e+02, 2.73398e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (150, 85), (517, 557)\n",
      "[DEBUG] Sign Space: [150, 85, 517, 557]\n",
      "[DEBUG] Detections: [tensor([[1.54084e+02, 1.31777e+02, 5.14025e+02, 6.01149e+02, 3.26989e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (150, 86), (517, 557)\n",
      "[DEBUG] Sign Space: [150, 86, 517, 557]\n",
      "[DEBUG] Detections: [tensor([[1.52859e+02, 1.32064e+02, 5.14454e+02, 6.01144e+02, 3.14062e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (150, 86), (517, 557)\n",
      "[DEBUG] Sign Space: [150, 86, 517, 557]\n",
      "[DEBUG] Detections: [tensor([[1.47503e+02, 1.19738e+02, 5.17651e+02, 6.06172e+02, 3.14823e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (150, 88), (517, 559)\n",
      "[DEBUG] Sign Space: [150, 88, 517, 559]\n",
      "[DEBUG] Detections: [tensor([[1.47681e+02, 1.18593e+02, 5.20071e+02, 6.09540e+02, 3.17047e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (150, 89), (517, 561)\n",
      "[DEBUG] Sign Space: [150, 89, 517, 561]\n",
      "[DEBUG] Detections: [tensor([[1.47681e+02, 1.18593e+02, 5.20071e+02, 6.09540e+02, 3.17047e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (150, 90), (517, 562)\n",
      "[DEBUG] Sign Space: [150, 90, 517, 562]\n",
      "[DEBUG] Detections: [tensor([[1.60460e+02, 9.93418e+01, 5.00182e+02, 5.12975e+02, 3.09518e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (149, 90), (518, 563)\n",
      "[DEBUG] Sign Space: [149, 90, 518, 563]\n",
      "[DEBUG] Detections: [tensor([[1.54025e+02, 1.28127e+02, 5.09479e+02, 5.45664e+02, 3.84668e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (149, 90), (520, 566)\n",
      "[DEBUG] Sign Space: [149, 90, 520, 566]\n",
      "[DEBUG] Detections: [tensor([[1.54025e+02, 1.28127e+02, 5.09479e+02, 5.45664e+02, 3.84668e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (149, 90), (520, 568)\n",
      "[DEBUG] Sign Space: [149, 90, 520, 568]\n",
      "[DEBUG] Detections: [tensor([[1.54047e+02, 1.28126e+02, 5.09470e+02, 5.45659e+02, 3.84749e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (149, 90), (520, 568)\n",
      "[DEBUG] Sign Space: [149, 90, 520, 568]\n",
      "[DEBUG] Detections: [tensor([[1.55558e+02, 1.10219e+02, 5.04967e+02, 5.07569e+02, 2.66429e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (148, 90), (521, 569)\n",
      "[DEBUG] Sign Space: [148, 90, 521, 569]\n",
      "[DEBUG] Detections: [tensor([[1.55579e+02, 1.09943e+02, 5.04757e+02, 5.07717e+02, 2.65279e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (148, 90), (521, 569)\n",
      "[DEBUG] Sign Space: [148, 90, 521, 569]\n",
      "[DEBUG] Detections: [tensor([[1.56743e+02, 1.05311e+02, 4.95849e+02, 5.04382e+02, 2.53373e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (148, 90), (521, 569)\n",
      "[DEBUG] Sign Space: [148, 90, 521, 569]\n",
      "[DEBUG] Detections: [tensor([[1.56573e+02, 1.05169e+02, 4.96030e+02, 5.04201e+02, 2.53042e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (148, 90), (521, 569)\n",
      "[DEBUG] Sign Space: [148, 90, 521, 569]\n",
      "[DEBUG] Detections: [tensor([[1.61975e+02, 1.12164e+02, 4.98197e+02, 5.51752e+02, 2.78584e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (148, 90), (521, 569)\n",
      "[DEBUG] Sign Space: [148, 90, 521, 569]\n",
      "[DEBUG] Detections: [tensor([[1.63792e+02, 1.11940e+02, 4.96497e+02, 5.50918e+02, 2.82346e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (148, 90), (521, 569)\n",
      "[DEBUG] Sign Space: [148, 90, 521, 569]\n",
      "[DEBUG] Detections: [tensor([[1.63686e+02, 1.11869e+02, 4.96562e+02, 5.51022e+02, 2.81747e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (148, 90), (521, 569)\n",
      "[DEBUG] Sign Space: [148, 90, 521, 569]\n",
      "[DEBUG] Detections: [tensor([[1.61123e+02, 1.14793e+02, 4.97689e+02, 5.47229e+02, 2.53583e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (149, 90), (520, 567)\n",
      "[DEBUG] Sign Space: [149, 90, 520, 567]\n",
      "[DEBUG] Detections: [tensor([[1.61107e+02, 1.14399e+02, 4.97680e+02, 5.47303e+02, 2.53400e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (150, 90), (519, 566)\n",
      "[DEBUG] Sign Space: [150, 90, 519, 566]\n",
      "[DEBUG] Detections: [tensor([[1.55120e+02, 9.64420e+01, 5.04874e+02, 5.18359e+02, 2.63552e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (151, 91), (520, 564)\n",
      "[DEBUG] Sign Space: [151, 91, 520, 564]\n",
      "[DEBUG] Detections: [tensor([[1.55120e+02, 9.64420e+01, 5.04874e+02, 5.18359e+02, 2.63552e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (152, 91), (519, 563)\n",
      "[DEBUG] Sign Space: [152, 91, 519, 563]\n",
      "[DEBUG] Detections: [tensor([[1.45882e+02, 1.19302e+02, 5.14849e+02, 5.58291e+02, 2.85142e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (151, 92), (518, 563)\n",
      "[DEBUG] Sign Space: [151, 92, 518, 563]\n",
      "[DEBUG] Detections: [tensor([[1.45366e+02, 1.19165e+02, 5.15421e+02, 5.58045e+02, 2.83407e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (152, 92), (517, 562)\n",
      "[DEBUG] Sign Space: [152, 92, 517, 562]\n",
      "[DEBUG] Detections: [tensor([], size=(0, 6))]\n",
      "[DEBUG] Không phát hiện người trong frame!\n",
      "[DEBUG] Detections: [tensor([], size=(0, 6))]\n",
      "[DEBUG] Không phát hiện người trong frame!\n",
      "[DEBUG] Detections: [tensor([], size=(0, 6))]\n",
      "[DEBUG] Không phát hiện người trong frame!\n",
      "[DEBUG] Detections: [tensor([[1.52962e+02, 1.28543e+02, 5.05668e+02, 5.51137e+02, 2.50357e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (152, 94), (517, 563)\n",
      "[DEBUG] Sign Space: [152, 94, 517, 563]\n",
      "[DEBUG] Detections: [tensor([[1.52962e+02, 1.28543e+02, 5.05668e+02, 5.51137e+02, 2.50357e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (152, 94), (517, 563)\n",
      "[DEBUG] Sign Space: [152, 94, 517, 563]\n",
      "[DEBUG] Detections: [tensor([[1.64102e+02, 7.39461e+01, 4.90595e+02, 4.19045e+02, 2.62628e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (153, 96), (516, 563)\n",
      "[DEBUG] Sign Space: [153, 96, 516, 563]\n",
      "[DEBUG] Detections: [tensor([[1.64102e+02, 7.39461e+01, 4.90595e+02, 4.19045e+02, 2.62628e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (153, 96), (516, 562)\n",
      "[DEBUG] Sign Space: [153, 96, 516, 562]\n",
      "[DEBUG] Detections: [tensor([[1.68737e+02, 7.50148e+01, 4.85241e+02, 4.19511e+02, 2.88771e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (154, 96), (515, 561)\n",
      "[DEBUG] Sign Space: [154, 96, 515, 561]\n",
      "[DEBUG] Detections: [tensor([[1.69529e+02, 7.68891e+01, 4.84075e+02, 4.18862e+02, 2.90668e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (154, 96), (515, 561)\n",
      "[DEBUG] Sign Space: [154, 96, 515, 561]\n",
      "[DEBUG] Detections: [tensor([[1.65542e+02, 7.53566e+01, 4.87132e+02, 4.23482e+02, 2.66377e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (154, 96), (515, 560)\n",
      "[DEBUG] Sign Space: [154, 96, 515, 560]\n",
      "[DEBUG] Detections: [tensor([[1.66339e+02, 7.63436e+01, 4.86214e+02, 4.22990e+02, 2.65665e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (154, 96), (515, 560)\n",
      "[DEBUG] Sign Space: [154, 96, 515, 560]\n",
      "[DEBUG] Detections: [tensor([[1.67639e+02, 7.60874e+01, 4.87428e+02, 4.19297e+02, 2.97980e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.68031e+02, 7.60944e+01, 4.86980e+02, 4.19374e+02, 3.00618e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.67017e+02, 7.56537e+01, 4.87643e+02, 4.19737e+02, 2.98917e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.68693e+02, 7.49400e+01, 4.86867e+02, 4.18092e+02, 3.10441e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.68419e+02, 7.43607e+01, 4.86597e+02, 4.17877e+02, 3.14209e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.63088e+02, 7.13274e+01, 4.89500e+02, 4.23306e+02, 2.69111e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.62987e+02, 7.14494e+01, 4.89551e+02, 4.23322e+02, 2.69226e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.64660e+02, 7.25457e+01, 4.87719e+02, 4.22213e+02, 2.70856e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.65366e+02, 7.42937e+01, 4.88131e+02, 4.19271e+02, 2.93068e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (155, 96), (516, 560)\n",
      "[DEBUG] Sign Space: [155, 96, 516, 560]\n",
      "[DEBUG] Detections: [tensor([[1.60895e+02, 7.12303e+01, 4.93997e+02, 4.20963e+02, 3.15404e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (156, 96), (515, 559)\n",
      "[DEBUG] Sign Space: [156, 96, 515, 559]\n",
      "[DEBUG] Detections: [tensor([[1.60996e+02, 7.11873e+01, 4.93148e+02, 4.21678e+02, 3.01824e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (156, 95), (515, 558)\n",
      "[DEBUG] Sign Space: [156, 95, 515, 558]\n",
      "[DEBUG] Detections: [tensor([[1.61165e+02, 7.25336e+01, 4.92738e+02, 4.20523e+02, 3.08935e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (156, 96), (515, 557)\n",
      "[DEBUG] Sign Space: [156, 96, 515, 557]\n",
      "[DEBUG] Detections: [tensor([[1.47739e+02, 7.85512e+01, 5.13523e+02, 5.33487e+02, 2.84969e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (156, 96), (515, 557)\n",
      "[DEBUG] Sign Space: [156, 96, 515, 557]\n",
      "[DEBUG] Detections: [tensor([[1.47813e+02, 7.79720e+01, 5.13478e+02, 5.33901e+02, 2.85248e-01, 0.00000e+00]])]\n",
      "[INFO] Sign Space Box: (156, 96), (515, 557)\n",
      "[DEBUG] Sign Space: [156, 96, 515, 557]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_video(\"E:/University/HK2_Nam3/PBL/clone_yolo/yolov3/videos_demo/demo_2.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
